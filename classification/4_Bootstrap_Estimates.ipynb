{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a5f83c-3fa0-416f-9a11-3bf26499c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Tools\n",
    "import os, sys, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import other classifier tools\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a7de87-ead6-470d-8aa6-dcd85268b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original data\n",
    "classification_dir = \"./Final_ResampledClassificationData.csv\"\n",
    "SelectedFeat_dir = \"./results/LASSO_SelectedFeatures.csv\"\n",
    "report_dir = \"./results\"\n",
    "\n",
    "# find all prediction files\n",
    "filelist = glob.glob(\"./results/classification/*_predictions_test.csv\")\n",
    "# print(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f0ca0d-b96e-4651-97cf-b2d1630a6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Data\n",
    "classify_df = pd.read_csv(classification_dir)\n",
    "features = pd.read_csv(SelectedFeat_dir)\n",
    "\n",
    "#Get Features\n",
    "ft = features[\"Feature\"].values.tolist()\n",
    "num_ft = len(ft)\n",
    "\n",
    "ft = [\"File\", \"unique_scandate\",\"unique_pt\",\"NE_Score\", \"NE_Status\", \"NE_Class\", \"Group\"] + ft\n",
    "feat_data = classify_df[ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be71de46-d99c-425f-9a21-5690f9615ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Group DataFrames\n",
    "train_df = feat_data[feat_data[\"Group\"] == \"train\"]\n",
    "val_df = feat_data[feat_data[\"Group\"] == \"val\"]\n",
    "test_df = feat_data[feat_data[\"Group\"] == \"test\"]\n",
    "merge_train = feat_data[feat_data[\"Group\"] != \"test\"]\n",
    "\n",
    "#Get Test Set variables \n",
    "X_test = test_df[test_df.columns[-num_ft:].values]\n",
    "Y_test = test_df[test_df.columns[5]]\n",
    "Y_files = test_df[\"File\"].tolist()\n",
    "Y_scans = test_df[\"unique_scandate\"].tolist()\n",
    "Y_pts = test_df[\"unique_pt\"].tolist()\n",
    "\n",
    "#Get Train and Val variables \n",
    "X_merge = merge_train[merge_train.columns[-num_ft:].values]\n",
    "Y_merge = merge_train[merge_train.columns[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e95b95-3c39-44b8-8c5e-955b282d3131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# create a matrix of 1000 bootstrap samples selecting only one series from each unique_scandate\n",
    "# we select this variable because our labels (NE status) are unique to each scan date (i.e. patients can have multiple labels across multiple scan dates)\n",
    "\n",
    "all_scandates = np.unique(Y_scans)\n",
    "print(len(all_scandates))\n",
    "\n",
    "num_bootstrap_samples = 1000\n",
    "bootstrap_samples = []\n",
    "np.random.seed(822)\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = []\n",
    "    # for all scan dates, randomly select one series\n",
    "    for _date in all_scandates:\n",
    "        condition = (test_df['unique_scandate'] == _date)\n",
    "        all_series = test_df.loc[condition, 'File']\n",
    "        _sample = np.random.choice(all_series.tolist(), size=1, replace=False)\n",
    "        bootstrap_sample.append(_sample.item())\n",
    "    bootstrap_samples.append(bootstrap_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aefd914-288d-4412-8329-a1a6301e3fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/classification\\Unweighted_LogisticRegression_Fold1_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_Fold2_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_Fold3_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_Fold4_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_Fold5_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Unweighted_LogisticRegression_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_Fold1_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_Fold2_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_Fold3_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_Fold4_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_Fold5_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Unweighted_RandomForest_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_Fold1_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_Fold2_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_Fold3_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_Fold4_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_Fold5_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Unweighted_XGBoost_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_auc_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_bal_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_LogisticRegression_F1_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_auc_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_bal_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_RandomForest_F1_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_auc_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_bal_SoftVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_Fold1_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_Fold2_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_Fold3_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_Fold4_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_Fold5_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_HardVote_Ensemble_predictions_test.csv\n",
      "./results/classification\\Weighted_XGBoost_F1_SoftVote_Ensemble_predictions_test.csv\n"
     ]
    }
   ],
   "source": [
    "bootstrap_metrics_dict = {\"model\":[],\"Accuracy_mean\":[], \"PPV_mean\":[], \"Sensitivity_mean\":[], \"NPV_mean\":[], \"Specificity_mean\":[], \"FPR_mean\":[], \"F1_Score_mean\":[], \"AUC_mean\":[],\n",
    "                         \"Accuracy_low95\":[], \"PPV_low95\":[], \"Sensitivity_low95\":[], \"NPV_low95\":[], \"Specificity_low95\":[], \"FPR_low95\":[], \"F1_Score_low95\":[], \"AUC_low95\":[],\n",
    "                         \"Accuracy_high95\":[], \"PPV_high95\":[], \"Sensitivity_high95\":[], \"NPV_high95\":[], \"Specificity_high95\":[], \"FPR_high95\":[], \"F1_Score_high95\":[], \"AUC_high95\":[]}\n",
    "\n",
    "for f in filelist:\n",
    "    print(f)\n",
    "    pred_df = pd.read_csv(f)\n",
    "    metrics_dict = {\"Accuracy\":[], \"PPV\":[], \"Sensitivity\":[], \"NPV\":[], \"Specificity\":[], \"FPR\":[], \"F1_Score\":[], \"AUC\":[]}\n",
    "    for _i in range(0,1000):\n",
    "        boot_df = pred_df[pred_df['File'].isin(bootstrap_samples[_i])]\n",
    "        Y_test = boot_df['gt'].tolist()\n",
    "        ypred = boot_df['pred_bin'].tolist()\n",
    "        ypred_prob = boot_df['pred_contin'].tolist()\n",
    "        \n",
    "        #calculate metrics  \n",
    "        acc = accuracy_score(Y_test, ypred)\n",
    "        ppv = precision_score(Y_test, ypred)\n",
    "        CM = confusion_matrix(Y_test, ypred)\n",
    "        f1 = f1_score(Y_test, ypred)\n",
    "        sens = recall_score(Y_test, ypred)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, ypred).ravel()\n",
    "        spec = tn / (tn+fp)\n",
    "        FPR = fp/(fp+tn)\n",
    "        try:\n",
    "            npv = tn/(tn + fn)\n",
    "        except:\n",
    "            npv = np.nan\n",
    "        \n",
    "        #append\n",
    "        metrics_dict[\"Accuracy\"].append(acc)\n",
    "        metrics_dict[\"PPV\"].append(ppv)\n",
    "        metrics_dict[\"Sensitivity\"].append(sens)\n",
    "        metrics_dict[\"Specificity\"].append(spec)\n",
    "        metrics_dict[\"FPR\"].append(FPR)\n",
    "        metrics_dict[\"F1_Score\"].append(f1)\n",
    "        metrics_dict[\"NPV\"].append(npv)\n",
    "        \n",
    "        #ROC AUC analysis\n",
    "        if \"HardVote\" not in f: \n",
    "            auc = roc_auc_score(Y_test, ypred_prob)\n",
    "        else: \n",
    "            auc = np.nan\n",
    "        metrics_dict[\"AUC\"].append(auc)\n",
    "        \n",
    "    # now calculate summary statistics from all 1000 bootstrap samples (mean and 95% CI)\n",
    "    acc_bs = metrics_dict[\"Accuracy\"]\n",
    "    ppv_bs = metrics_dict[\"PPV\"]\n",
    "    sens_bs = metrics_dict[\"Sensitivity\"]\n",
    "    spec_bs = metrics_dict[\"Specificity\"]\n",
    "    fpr_bs = metrics_dict[\"FPR\"]\n",
    "    f1_bs = metrics_dict[\"F1_Score\"]\n",
    "    npv_bs = metrics_dict[\"NPV\"]\n",
    "    auc_bs = metrics_dict[\"AUC\"]\n",
    "\n",
    "    # add to final saved dictionary\n",
    "    bootstrap_metrics_dict[\"model\"].append(f)\n",
    "    bootstrap_metrics_dict[\"Accuracy_mean\"].append(np.nanmean(acc_bs))\n",
    "    bootstrap_metrics_dict[\"Accuracy_low95\"].append(np.nanquantile(acc_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"Accuracy_high95\"].append(np.nanquantile(acc_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"PPV_mean\"].append(np.nanmean(ppv_bs))\n",
    "    bootstrap_metrics_dict[\"PPV_low95\"].append(np.nanquantile(ppv_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"PPV_high95\"].append(np.nanquantile(ppv_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"Sensitivity_mean\"].append(np.nanmean(sens_bs))\n",
    "    bootstrap_metrics_dict[\"Sensitivity_low95\"].append(np.nanquantile(sens_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"Sensitivity_high95\"].append(np.nanquantile(sens_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"Specificity_mean\"].append(np.nanmean(spec_bs))\n",
    "    bootstrap_metrics_dict[\"Specificity_low95\"].append(np.nanquantile(spec_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"Specificity_high95\"].append(np.nanquantile(spec_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"FPR_mean\"].append(np.nanmean(fpr_bs))\n",
    "    bootstrap_metrics_dict[\"FPR_low95\"].append(np.nanquantile(fpr_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"FPR_high95\"].append(np.nanquantile(fpr_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"NPV_mean\"].append(np.nanmean(npv_bs))\n",
    "    bootstrap_metrics_dict[\"NPV_low95\"].append(np.nanquantile(npv_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"NPV_high95\"].append(np.nanquantile(npv_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"F1_Score_mean\"].append(np.nanmean(f1_bs))\n",
    "    bootstrap_metrics_dict[\"F1_Score_low95\"].append(np.nanquantile(f1_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"F1_Score_high95\"].append(np.nanquantile(f1_bs, 0.975))\n",
    "    bootstrap_metrics_dict[\"AUC_mean\"].append(np.nanmean(auc_bs))\n",
    "    bootstrap_metrics_dict[\"AUC_low95\"].append(np.nanquantile(auc_bs, 0.025))\n",
    "    bootstrap_metrics_dict[\"AUC_high95\"].append(np.nanquantile(auc_bs, 0.975))\n",
    "\n",
    "b_df = pd.DataFrame(bootstrap_metrics_dict)\n",
    "b_df.to_csv(os.path.join(report_dir, \"bootstrap_test_all_models.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda51d38-aa61-4756-af0c-f8199950ea79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e14f5-b62c-434f-b2c0-0bc291df7ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537207a-b471-4208-b4d1-4034eed8624b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
